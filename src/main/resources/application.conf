rag-builder {
  // Ollama settings
  ollama {
    host = "http://127.0.0.1:11434"
    // Use the environment variable if available, otherwise default to localhost
    host = ${?OLLAMA_HOST}
    embedding-model = "mxbai-embed-large"
    chat-model = "llama3.1:8b"
    query = "What are these documents about?"
    timeout = 5 // This is the max time to wait for a response from ollama in minutes
    topResults = 4 // This is how many chunks to get from a query of the lucene index
  }

  // Text chunking parameters
  chunker {
    max-chars = 1800
    overlap = 250
  }

  input {
    pdfs = "input_pdfs.txt"
  }

  mapReduce {
    numReduceJobs = 4 // Number of reducers
  }

  output {
    outputDir = "outDir"
    mergeDir = "mergeDir"
  }

  word-relations {
      similarities = [
        ["function", "variable"],
        ["type", "variable"],
        ["addition", "deletion"],
        ["program", "software"],
        ["AST", "structure"]
      ]

      analogies = [
        ["function", "variable", "type"],
        ["addition", "deletion", "change"],
        ["program", "software", "repository"],
        ["AST", "structure", "diff"]
      ]
    }
}